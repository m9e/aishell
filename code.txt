// user_interface.py
import os


class UserInterface:
    def __init__(self):
        self.interactive_mode = False

    def get_instruction(self):
        return input("Enter instruction: ")

    def get_question(self):
        return input("Enter question: ")

    def get_limit(self):
        return int(input("Enter new execution limit: "))

    def display_command(self, command):
        print(f"Generated command: {command}")

    def display_answer(self, answer):
        print(f"Answer: {answer}")

    def display_message(self, message):
        print(message)

    def toggle_interactive_mode(self):
        self.interactive_mode = not self.interactive_mode
        print(f"Interactive mode {'enabled' if self.interactive_mode else 'disabled'}.")

    def confirm_execution(self):
        return input("Execute command? (y/n): ").lower() == 'y'

// aishell.py
#!/usr/bin/env python3

import os
import sys
import termios
import tty
import signal
from terminal_controller import TerminalController
from llm_interface import LLMInterface
from command_executor import CommandExecutor
from user_interface import UserInterface
from context_manager import ContextManager

def main():
    # Initialize components
    llm_interface = LLMInterface()
    command_executor = CommandExecutor()
    context_manager = ContextManager()
    user_interface = UserInterface()

    # Create the terminal controller
    with TerminalController() as controller:
        # Main loop
        while True:
            try:
                char = controller.getch()
                if char == b'\x09':  # Ctrl-I
                    command = controller.handle_ctrl_i()
                    if command == 'n':
                        instruction = user_interface.get_instruction()
                        llm_response = llm_interface.generate_command(instruction)
                        user_interface.display_command(llm_response)
                        if not user_interface.interactive_mode:
                            command_executor.execute(llm_response)
                        else:
                            if user_interface.confirm_execution():
                                command_executor.execute(llm_response)
                    elif command == 's':
                        user_interface.display_message("LLM mode stopped.")
                    elif command == 'a':
                        question = user_interface.get_question()
                        context = context_manager.get_context()
                        answer = llm_interface.answer_question(question, context)
                        user_interface.display_answer(answer)
                    elif command == 'l':
                        limit = user_interface.get_limit()
                        command_executor.set_limit(limit)
                    elif command == 'i':
                        user_interface.toggle_interactive_mode()
                else:
                    # Normal terminal operation
                    os.write(sys.stdout.fileno(), char)
            except KeyboardInterrupt:
                # Handle Ctrl-C
                os.write(sys.stdout.fileno(), b'^C\n')
            except EOFError:
                # Handle Ctrl-D
                break

if __name__ == "__main__":
    main()

// context_manager.py
from collections import deque

class ContextManager:
    def __init__(self, max_lines=500):
        self.context = deque(maxlen=max_lines)

    def add_line(self, line):
        self.context.append(line)

    def get_context(self):
        return "\n".join(self.context)

// terminal_controller.py
import os
import sys
import termios
import tty

class TerminalController:
    def __init__(self):
        self.old_settings = termios.tcgetattr(sys.stdin)

    def __enter__(self):
        tty.setraw(sys.stdin.fileno())
        return self

    def __exit__(self, type, value, traceback):
        termios.tcsetattr(sys.stdin, termios.TCSADRAIN, self.old_settings)

    def getch(self):
        return os.read(sys.stdin.fileno(), 1)

    def handle_ctrl_i(self):
        sys.stdout.write("\nInstruction? (n/s/a/l/i): ")
        sys.stdout.flush()
        return self.getch().decode('utf-8')

// test_aishell.py
import sys
import pytest
from unittest.mock import Mock, patch
import os
from collections import deque

# Imports (keep them as they are in your current file)
from context_manager import ContextManager
from llm_interface import LLMInterface
from command_executor import CommandExecutor
from user_interface import UserInterface

# Fixtures
@pytest.fixture
def context_manager():
    return ContextManager(max_lines=5)

@pytest.fixture
def mock_azure_client():
    with patch('llm_interface.AzureOpenAI') as mock_azure:
        mock_client = Mock()
        mock_azure.return_value = mock_client
        yield mock_client

@pytest.fixture
def llm_interface(mock_azure_client):
    return LLMInterface()

@pytest.fixture
def command_executor():
    return CommandExecutor()

@pytest.fixture
def user_interface():
    return UserInterface()

# Tests for ContextManager
def test_context_manager_add_and_get(context_manager):
    context_manager.add_line("Line 1")
    context_manager.add_line("Line 2")
    assert context_manager.get_context() == "Line 1\nLine 2"

def test_context_manager_max_lines(context_manager):
    for i in range(10):
        context_manager.add_line(f"Line {i}")
    assert len(context_manager.context) == 5
    assert context_manager.get_context().startswith("Line 5")

# Tests for LLMInterface
def test_llm_interface_generate_command(llm_interface, mock_azure_client):
    # Create a mock response that mimics the structure of the actual API response
    mock_response = Mock()
    mock_response.choices = [Mock(message=Mock(content="echo 'Hello, World!'"))]
    mock_azure_client.chat.completions.create.return_value = mock_response
    
    command = llm_interface.generate_command("Print Hello World")
    assert command == "echo 'Hello, World!'"

def test_llm_interface_answer_question(llm_interface, mock_azure_client):
    # Create a mock response that mimics the structure of the actual API response
    mock_response = Mock()
    mock_response.choices = [Mock(message=Mock(content="Sunny"))]
    mock_azure_client.chat.completions.create.return_value = mock_response
    
    answer = llm_interface.answer_question("What's the weather?", "Context: It's a clear day.")
    assert answer == "Sunny"

def test_llm_interface_error_handling(llm_interface, mock_azure_client):
    mock_azure_client.chat.completions.create.side_effect = Exception("API Error")
    command = llm_interface.generate_command("This will fail")
    assert command is None

# Tests for CommandExecutor
@patch('subprocess.run')
def test_command_executor_execute(mock_run, command_executor):
    mock_run.return_value.stdout = "Command output"
    command_executor.execute("echo 'test'")
    mock_run.assert_called_once_with("echo 'test'", shell=True, check=True, text=True, capture_output=True)

def test_command_executor_limit(command_executor):
    command_executor.set_limit(2)
    for _ in range(3):
        with patch('subprocess.run') as mock_run:
            command_executor.execute("test")
    assert command_executor.execution_count == 2

# Tests for UserInterface
@patch('builtins.input')
@patch('builtins.print')
def test_user_interface_get_instruction(mock_print, mock_input, user_interface):
    mock_input.return_value = "list files"
    instruction = user_interface.get_instruction()
    assert instruction == "list files"

@patch('builtins.print')
def test_user_interface_display_command(mock_print, user_interface):
    user_interface.display_command("ls -la")
    mock_print.assert_called_once_with("Generated command: ls -la")

def test_user_interface_toggle_interactive_mode(user_interface):
    assert not user_interface.interactive_mode
    user_interface.toggle_interactive_mode()
    assert user_interface.interactive_mode
    user_interface.toggle_interactive_mode()
    assert not user_interface.interactive_mode

# Integration test
def test_integration_generate_and_execute():
    llm_interface = LLMInterface()
    command_executor = CommandExecutor()
    user_interface = UserInterface()

    with patch.object(llm_interface, 'generate_command', return_value="echo 'Integration Test'"):
        with patch.object(command_executor, 'execute') as mock_execute:
            instruction = "Run an integration test"
            llm_response = llm_interface.generate_command(instruction)
            user_interface.display_command(llm_response)
            command_executor.execute(llm_response)

            mock_execute.assert_called_once_with("echo 'Integration Test'")

# Environmental variable test
def test_environment_variables():
    required_vars = ['AZURE_OPENAI_API_KEY', 'AZURE_OPENAI_ENDPOINT', 'AZURE_OPENAI_DEPLOYMENT_NAME']
    for var in required_vars:
        assert os.getenv(var) is not None, f"Environment variable {var} is not set"

@pytest.mark.integration
def test_real_api_call():
    llm_interface = LLMInterface()
    
    # Test generate_command
    try:
        command = llm_interface.generate_command("List all files in the current directory")
        assert command is not None
        assert isinstance(command, str)
        print(f"\nGenerated command: {command}", file=sys.stderr)
        
    except Exception as e:
        pytest.fail(f"generate_command failed with error: {str(e)}")
    
    # Test answer_question
    try:
        answer = llm_interface.answer_question("What's the capital of France?", "")
        assert answer is not None
        assert isinstance(answer, str)
        print(f"\nAnswer to 'What's the capital of France?': {answer}", file=sys.stderr)
        
    except Exception as e:
        pytest.fail(f"answer_question failed with error: {str(e)}")




if __name__ == "__main__":
    pytest.main([__file__])

// command_executor.py
import subprocess

class CommandExecutor:
    def __init__(self):
        self.limit = None
        self.execution_count = 0

    def execute(self, command):
        if self.limit and self.execution_count >= self.limit:
            print("Execution limit reached. Use 'Ctrl-I l' to set a new limit.")
            return

        try:
            result = subprocess.run(command, shell=True, check=True, text=True, capture_output=True)
            print(result.stdout)
            self.execution_count += 1
        except subprocess.CalledProcessError as e:
            print(f"Error executing command: {e}")

    def set_limit(self, limit):
        self.limit = limit
        self.execution_count = 0

// __init__.py


// llm_interface.py
import os
from openai import AzureOpenAI

class LLMInterface:
    def __init__(self):
        self.client = AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
            api_version="2023-12-01-preview",
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
        )
        self.deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

    def generate_command(self, instruction):
        try:
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "You are an AI assistant that generates bash commands."},
                    {"role": "user", "content": f"Generate a bash command for: {instruction}"}
                ]
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error generating command: {e}")
            return None

    def answer_question(self, question, context):
        try:
            response = self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "You are an AI assistant that answers questions based on given context."},
                    {"role": "user", "content": f"Context: {context}\n\nQuestion: {question}"}
                ]
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error answering question: {e}")
            return None

